---
title: "Appendix B: Re-analysis of original data (Shebani and Pulvermüller, 2013)"
author: '[Guillermo Montero-Melis](https://www.mpi.nl/people/montero-melis-guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
---


Introduction
============

This report presents a re-analysis of the original data from the study
Shebani and Pulvermüller (2013, *Cortex*, hereafter SP13). In the script we:

- Plot and summarize the data to check we obtain the same summaries as in SP13;
- Replicate the same ANOVAs reported in the original; we emphasize that an
**ANOVA analysis is inappropriate for this type of error count data** and that it
may lead to over-estimation of the effect size (see our manuscript and Jaeger,
2008).
- Try to figure out how SP13 arrived at the effect size measure they report;
- Re-analyze the data with more appropriate statistical methods.


*NB*:

- The original data have been made publicly available by the original authors
at https://github.com/zshebani/LMB/tree/1.0
(DOI: 10.5281/zenodo.3402035).
- Some of the analyses and simulations run below take up a considerable amount of
computation time. In these cases we have saved the output of the code to disk (to
the "data/" subfolder) so that it can later be read without the need to re-run
the analyses. If you wish to re-run the analyses, please uncomment the relevant
code.
- For R version, loaded packages and their versions, etc., see session info at
the end of this document.


Setup workspace
==============

Load libraries


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library("knitr")
library("lme4")
library("brms")
library("tidyverse")  # ggplot2, dplyr, readr, purrr, etc
library("boot")       # for inv.logit()
library("DescTools")  # EtaSq()
library("lsr")        # cohensD()
library("sjPlot")     # tab_model()
# library("sjstats")    # eta_sq()
```


Load original data set:

```{r, message=FALSE}
# Load data
d_allconds <- read_csv("data/SP13_orig-data_total-errors_long-format.csv")
d_allconds$subject <- factor(d_allconds$subject)
```



Describe and plot the data set
====================

```{r}
kable(head(d_allconds))
```

- The data frame shows the total number of errors, adding all three types of errors:
omissions, replacements, and transpositions.
- This data set contains the data for all four movement conditions:
`r unique(d_allconds$movement)`. However, we are only interested in the two
critical arm/leg movement conditions ("arm_paradi" and "leg_paradi"), so we
subset the relevant data:

```{r}
d <- d_allconds %>% filter(movement %in% c("arm_paradi", "leg_paradi"))
```


Plot the data:

```{r, echo = FALSE}
ggplot(d, aes(x = word_type, y = errors, colour = word_type)) +
  geom_boxplot() +
  facet_grid(. ~ movement)
```

We can qualitatively see the cross-over interaction. Note that although the
variability is large, a hypothetical effect may still be detected if there is
sufficient consistency within subjects, as it is a within-subjects design. This
can be better appreciated in the following plot:

```{r, include = TRUE, echo = FALSE}
# basic plot
plot_orig <- d %>%
  mutate(sbj_wtype = paste(subject, word_type, sep = "_")) %>%
  mutate(movement  = ifelse(movement == "arm_paradi", "arm\nmovements", "leg\nmovements"),
         word_type = ifelse(word_type == "arm", "arm words", "leg words")) %>%
  ggplot(aes(x = movement, colour = word_type, y = errors)) +
  stat_summary(fun.y = mean, geom = "point", size = 4,
               position = position_dodge(width = .25)) +
  stat_summary(fun.y = mean, geom = "line", aes(group = word_type), size = 2,
               position = position_dodge(width = .25)) +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, size = 2,
               position = position_dodge(width = .25)) +
  xlab("") + ylab("Number of errors") +
  theme_classic() +
  theme(legend.title = element_blank(),
        legend.position="top")
```

```{r, echo = FALSE}
# Plot for report
plot_orig +
  geom_jitter(height = 0, width = .05, alpha = .5) +
  geom_line(aes(group = sbj_wtype), alpha = .5)
```


The thick dots and lines show mean number of errors and bootstrapped
(non-parametric) 95% CIs.
The thinner dots and lines show subject data. There seems to be quite a lot of
consistency by subjects, i.e. the slopes of the thin lines mostly go in the same
direction as the slope of the thicker lines.


```{r, include=FALSE}
# Create a png version to add in the paper:
if(! dir.exists("myfigures")) dir.create("myfigures")
ggsave("myfigures/fig_original_results.png",
       plot_orig + theme(text = element_text(size=18)),
       width = 4.5, height = 3)
```


Or we can directly look at the interaction effect by comparing errors in 
matching conditions (when effector and semantic content are the same, i.e.
arm-arm, leg-leg) and mismatching conditions (ffector and semantic content are
different, e.g. arm-leg, leg-arm):


```{r}
d$eff_word <- ifelse(substr(d$movement,1,3) == d$word_type, "match", "mismatch")
kable(head(d, 4))
d_interact <- d %>% 
  group_by(subject, eff_word) %>%
  summarise(errors = mean(errors))
kable(head(d_interact, 2))
```

We can now plot this by-subject interaction effect:

```{r, echo = FALSE}
ggplot(d_interact, aes(x = eff_word, y = errors)) +
  geom_jitter(height = 0, width = .05, alpha = .5) +
  geom_line(aes(group = subject), alpha = .5) +
  stat_summary(fun.y = mean, geom = "point", size = 4,
               position = position_dodge(width = .25)) +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, size = 2,
               position = position_dodge(width = .25)) +
  xlab("Effector-Word type") +
  ggtitle("Interference effect")
```


Lines show individual participants. Downward slopes indicate that a participant
numerically showed the interference effect.


Replicate originally reported analysis with ANOVAs (statistically inappropriate analysis)
==================================================


## Data summaries

First, let's see if our data summaries look like those reported in Table 2 of
SP13 (p.226):

```{r}
d_allconds %>%
  group_by(movement, word_type) %>%
  summarise(
    M  = mean(errors),
    SD = sd(errors)
    ) %>%
  kable(digits = 2)
```

Yes, this replicates the values in Table 2 of SP13!


## Anova on full 4 (conditions) x 2 (word categories) design:

Can we replicate the results reported in SP13 (p.225)?

```{r}
# Following this post:
# https://www.r-bloggers.com/two-way-anova-with-repeated-measures/
aov_orig_allconds <- aov(
  errors ~ movement * word_type + Error(subject / (movement * word_type)),
  data = d_allconds
  )
summary(aov_orig_allconds)
```

Yes, pretty much: The *F*- and *p*-values are identical. What differs are the
MSE and I am not sure how they were computed in SP13.



## Anova on critical 2 x 2 design

Most importantly, can we replicate the critical results
"directly addressing the main hypothesis motivating this study" (SP13, p.226)?

```{r}
aov_orig <- aov(
  errors ~ movement * word_type + Error(subject / (movement * word_type)),
  data = d
  )
summary(aov_orig)
```

Yes: The *F*-values, degrees of freedom and *p*-values are identical.
(As before, however, their reported MSE differs from what we obtain.)


## Effect size: Where does the originally reported Cohen's $d$ come from?

The authors report Cohen's $d = 1.25$ as the effect size for the critical
interaction (p.226). However, since Cohen's *d* is not an effect size measure
that is commonly reported in the context of ANOVAs, it is not clear how they
arrived at this figure.
Below we try to reproduce it.



### Does the *d* value come from a t-test?

What if we compare the average difference scores corresponding to the 
interaction effect, i.e. we run a t-test on the interaction effect we plotted
above (match vs mismatch):

```{r}
d_4ttest <- spread(d_interact, eff_word, errors)
head(d_4ttest, 3)
with(d_4ttest, t.test(match, mismatch, paired = TRUE))
```

The t-test comes out significant (but bear in mind that it is not the
appropriate analysis, see below). We can compute Cohen's $d$ for this test.

Cohen's $d$:

```{r}
# Following https://rcompanion.org/handbook/I_04.html
with(d_4ttest, (mean(match) - mean(mismatch)) / sd(match - mismatch))
# Note we get the same value using the lsr package:
# with(d_4ttest, lsr::cohensD(mismatch, match, method = "paired"))
```

```{r, include=FALSE}
d_09 <- round(with(d_4ttest, (mean(match) - mean(mismatch)) / sd(match - mismatch)), 2)
```

No; we obtain a Cohen's $d = `r d_09`$, unlike the figure reported in SP13.


### Computing $d$ from (partial) $\eta^2$?

Eta squared ($\eta^2$) is the most commonly used measure of effect size for
ANOVAs: It "measures the proportion of the total variance in a dependent
variable that is associated with [...] an independent variable" (Richardson,
2011, p. 135; for an example calculation, see 
[here](https://psychohawks.wordpress.com/2010/10/31/effect-size-for-analysis-of-variables-anova/)).

```{r}
eta_sq <- DescTools::EtaSq(aov_orig, type = 1)
eta_sq
# sjstats::eta_sq(aov_orig)  # yields same results
```

The value for the critical movement--word type interaction of interest is one of
two:

a) $\eta^2 = `r round(eta_sq[3,1], 2)`$
b) partial $\eta^2 = `r round(eta_sq[3,2], 2)`$.


We tried two methods of converting (partial) $\eta^2$ to Cohen's $d$ and we
computed two values with each method, once for $\eta^2$ and once for partial
$\eta^2$:

1. [This online calculator](https://www.psychometrica.de/effect_size.html)
converts between different effect size measures (see section 13):
    a) When plugging in $\eta^2 = `r round(eta_sq[3,1], 2)`$, the corresponding
    effect size is $d = 0.41$, which is far off from the figure reported in SP13.
    b) When plugging in partial $\eta^2 = `r round(eta_sq[3,2], 2)`$, the 
    corresponding effect size is $d = 1.92$, which is also different from that
    reported in the original (but in this case, it is greater).
2. [This IBM support website](https://www-01.ibm.com/support/docview.wss?uid=swg21476421)
cites formulae to convert between $\eta^2$, Cohen's $f$, and
Cohen's $d$: 
\[
f = \sqrt{ \frac{\eta^2}{1 - \eta^2} };
d = 2*f
\]
This formula yields exactly the same results as above:
    a) $d=0.41$ if computed from the  $\eta^2$,
    b) $d=1.92$ if computed from the partial $\eta^2$.


Conclusion
----------

While we are able to replicate the descriptive statistics and the ANOVA results
reported in SP13 from the data the authors have shared, we are unable to
reproduce their reported effect size value of Cohen's $d = 1.25$ (SP13, p. 226).
The two methods we have tried yield either $d=`r d_09`$, $d=0.41$, or $d=1.92$.


Treat data as coming from a binomial distribution, but what is $n$?
======================================================

SP13 analyzed their data using ANOVAs and t-tests. We instead want to model the
dependent variable (DV = number of errors per experimental cell) as coming from
a binomial distribution: 

$$errors \sim B(n, p)$$

Here, $n$ is the upper bound on the possible number of errors and $p$ is the
probability of making an error for each word.
We will assume that the maximum number of errors for each trial (i.e., sequence
of 4 words) is 4.
We also assume that the probability of making an error was the same
for all words (we have to do this in our re-analysis as the shared data are
aggregated across items).
Crucially, we also need to determine how many trials there were per cell and
this turns out to be tricky, because SP13 report this inconsistently (see below).

We emailed the original authors about this. Here is the relevant extract, with 
the authors' responses as inline comments (email correspondence with Z. Shebani
from 1 Apr 2018):

> **Number of trials**
>
> It says that 24 trials were presented in each block, twelve arm-word and 12 leg-word trials (sect. 2.3, p.225 left
column [=LC]). It also says that 4 words were presented per trial (sect. 2.3, p.224 right column [=RC]). This implies
that 48 words of each category were shown per block. Since the lists consist of 36 words per category, the above
would suggest that some words were repeated in each block (e.g., 12 words per category repeated once), but this is
not explicitly stated. Was this the case?
>
>> Yes, 48 words from each category were shown in each block. Twelve words per category, randomly selected, were
repeated once in each block.
>
> It then says that conditions were run as separate blocks (sect. 2.3, p.225 LC) and that the full set of 72 words were
presented twice in each condition (sect. 2.3, p.225 RC). These figures don’t seem to add up: For 72 * 2 = 144 words
to be presented, this would require 144 / 4 = 36 trials (assuming 4 words are presented in each trial). In other words,
one would need 1.5 blocks per condition, whereas in the study it says that “the conditions were run as separate
blocks with twenty-four trials in each block” (p.225). Is there perhaps a typo in the reported numbers, or do we err
in our reasoning? In sum, what was the exact number of words per trial, trials per block (we assume equal number
of leg- and arm-word trials), and blocks per condition?
>
>> Line 2 of page 225, RC does indeed contain a typo. As mentioned above, 48 words from each category were
presented in each block (36 +12 = 48), therefore, not all words were repeated twice in each block/condition.
>>
>> In sum:
>>
>> Words per trial: 4
>>
>> Trials per block: 24 (12 arm word and 12 leg word trials)
>>
>> Blocks in the experiment: 4
>>
>> Blocks per condition: 1


**Conclusion**

Based on this correspondence, we take there to be 12 trials per experimental
cell. Since we assume the maximum number of errors per trial is 4,
$n = 12 \times 4 = 48$.

The basic probabilistic model is
$$errors \sim B(48, p)$$
and we are trying to estimate *p*.

```{r}
d$n <- 48
```



Bayesian binomial mixed model with *brms* (appropriate re-analysis)
========================================

We fit a binomial model using the same priors as we are going to use in our
replication:

- Weakly informative priors for all population-level fixed effects coefficients:
$N(0,\sigma^2 = 4)$;
- `brms`'s default priors for the variance components (random effects).


Preparations
-----------

First, set coding scheme to contrast coding:

```{r}
# We use numeric coding for consistency with the simulations in Appendix C.
# (Numeric coding allows to remove correlation terms in random effects.)
d$mv <- scale(as.numeric(factor(d$movement)), scale = FALSE) * -2
d$wt <- scale(as.numeric(factor(d$word_type)), scale = FALSE) * -2
d$mv_wt <- d$mv * d$wt
# Note that this coding exactly replicates contrast coding obtained with
# contr.sum(2) on a factor (arm levels = 1, leg levels = -1):
kable(head(d, 4))
```


See which priors can be specified for this model and what defaults there are?

```{r}
get_prior(
  errors | trials(n) ~ 1 + mv * wt + (1 + mv * wt | subject),
  data = d,
  family = "binomial"
  )
```

Specify weakly informative priors $N(0,\sigma^2 = 4)$ for population-level fixed
effects:

```{r}
myprior <- set_prior("normal(0, 2)", class = "b")  
# NB: In Stan a normal distribution is specified with sigma (*not* sigma^2), see
# https://mc-stan.org/docs/2_18/functions-reference/normal-distribution.html
# and
# https://stackoverflow.com/questions/52893379/stan-in-r-standard-deviation-or-variance-in-normal-distribution
```


Fit the models
-------------

The following function fit pairs of models. One is the *full* model: it contains
the population-level interaction effect of interest (movement-by-word type).
The other is the *null* model: it is in all respects identical to the full, 
except that it does not contain the population-level critical interaction.

(NB: The function takes the formula of the full model as input.)

```{r}
# Function to fit brms full and null models given a formula and save them to
# disk with appropriate name defined by RE (string argument):
fit_brm <- function(formu_str, RE, df = d) {
  f <- as.formula(formu_str)
  full <- brm(formula = f, data = df, prior = myprior, family = "binomial",
              iter = 15000, warmup = 2000, chains = 4, save_all_pars = TRUE)
  null <- update(full, ~ . -mv_wt, save_all_pars = TRUE)
  BF <- brms::bayes_factor(full, null)
  l <- list(full, null, BF)
  saveRDS(l, paste("data/sp13_bfm_", RE, ".rds", sep = ""))
  l
}
```


Fit and save:

```{r}
# # Fit maximal RE models (in the spirit of Barr et al., 2013)
# bfm_max <- fit_brm(
#   formu_str = "errors | trials(n) ~ 1 + mv + wt + mv_wt + (1 + mv + wt + mv_wt | subject)",
#   RE = "max"
#   )
```

Load from disk (once it has been run):

```{r}
bfm_max <- readRDS("data/sp13_bfm_max.rds")
# It's a list with three objects: the full model, the null model, and the BF
# for the interaction based on bridge sampling. Store the full model separately:
bfm_full <- bfm_max[[1]]
```


Summary and interpretation of the full model
------------------------------

```{r}
summary(bfm_full)
```



**Interpretation (backtransforming to odds)**:

- The estimated average odds of an error (intercept) is 
$e ^ {`r round(fixef(bfm_full)[1, 1], 2)`} = `r round(exp(fixef(bfm_full)[1, 1]), 2)`$
(corresponding to an average probability of
`r round(inv.logit(fixef(bfm_full)[1, 1]), 2)` of making an error for any word).
- Neither the type of movement (`mv`) nor the type of word (`wt`) have a
significant effect on the odds of making an error.
- The critical interaction (`mv_wt`), however, is significant (see next point)
and tells us that the odds of making an error if the effector of the movement 
*coincides* with the word type (as opposed to when the two differ) is
$e ^ {`r round(fixef(bfm_full)[4,1], 2)`} =`r round(exp(fixef(bfm_full)[4,1]), 2)`$.
In other words, participants were `r round(exp(fixef(bfm_full)[4,1]), 2)` times
more likely to make an error if effector and semantics match than if they mismatch.
This is the *interference effect* of interest!
- The *brms* model features 95% credible intervals for parameter estimates.
The 95% credible interval for the critical interaction is
$[`r round(fixef(bfm_full)[4,3], 2)`, `r round(fixef(bfm_full)[4,4], 2)`]$
(in log-odds). Note that zero is not contained in this interval: that is why
the effect can be considered significant.



Is the effect supported by the data? (Bayes factor)
-------------------------------

A more stringent way to evaluate the significance of the interaction effect
is to use Bayes factors (BF). In this case we evaluate two models, the full
and the null models fitted (and loaded) above.

There are different ways to compute Bayes factors. We used bridge sampling, 
which is the one recommended in [this](https://rpubs.com/lindeloev/bayes_factors)
post by Jonas Kristoffer Lindeløv (and also by 
[Paul Buerkner](https://twitter.com/paulbuerkner/status/963585470482604033?lang=en),
the developer of `brms`, himself).
It is also the method to compute BFs that we will use for our own study

```{r}
# It's the 3rd element in the list bfm_max
# Once run, load from disk:
BF_bfm <- bfm_max[[3]]
BF_bfm
```


This BF constitutes only inconclusive or anecdotical evidence in favour of the
alternative hypothesis ($H_1$) against the null ($H_0$): $1< BF_{10} < 6$.


Posterior densities
------------------

`brms` also lets us plot the posterior distribution of the model estimates. The
posterior distribution for the interfefence effect is shown in the 4th panel
of the left column.

```{r, fig.height=7}
plot(bfm_full)
```



Sanity check: GLMM with *lme4*
================

As a sanity check, we also fit a frequentist version of the GLMM above with the
*lme4* package. Do we roughly obtain the same estimates?


We fit an equivalent model to the one above:

```{r}
fm_binom <- glmer(
  cbind(errors, n - errors) ~ 1 + mv + wt + mv_wt + (1 + mv + wt + mv_wt | subject),
  data = d, 
  family = "binomial"
  )
```

The `boundary (singular) fit` warning is probably due an overspecified model,
as we only have four observations per participant but we try to estimate random
effect slopes for the 2x2 design. The estimates can nevertheless be trusted
(P. Alday, personal communication, 17 Dec 2019). As shown below, the model
estimates in the `lme4` are also very similar to those from `brms`, which
constitutes at least one type of validation since two different methods yield
very similar results.

```{r}
summary(fm_binom)
```


The point estimates of the `lme4` and `brms` models are almost identical:

```{r}
tibble(
  Coefficient = rownames(fixef(bfm_full)),
  brms = fixef(bfm_full)[, 1],
  lme4 = fixef(fm_binom)
) %>%
  kable(digits = 3)
```

Note that the estimated SEMs for the fixed-effects coefficients also are quite
comparable. This shows that our results are not specific to using Bayesian
GLMMs (in `brms`).


Conclusion
==========

The interference effect is suggested by the evaluation of the critical
coefficient (movement-by-word type interaction) in the full model.
The effect size of this interference effect is rather small:
`r round(fixef(bfm_full)[4, 1], 2)` log-odds. This can be compared to the
guidelines proposed by Chen, Cohen, and Chen (2010), where an effect of 0.5
log-odds would be considered a "small" effect. The current effect being
less than one third of this could even be considered very small.

Importantly, the BF analysis does only yield "anecdotical" or "inconclusive"
evidence for the existence of the effect.



Session info
============

```{r}
sessionInfo()
```

